{"posts":[{"title":"PostgreSQL(PostGIS)安装与卸载","text":"PostgreSQL(PostGIS)安装第一步：安装PostgreSQL运行postgresql-11.10-1-windows-x64.exe。 选择安装目录，我的是”E:\\PostgreSQL\\11”。 全选，next。 存储路径选择默认路径，next。 为数据库默认超级账户 postgres 设置密码,next。 选择默认端口5432，next。 区域选择：默认，一直next，等待安装完毕。 是否运行拓展下载工具，取消勾选，Finish,安装完毕。 第二步：安装PostGIS拓展运行postgis-bundle-pg11x64-setup-3.0.2-1.exe。 勾选上Create spatial database，next。 选择PostgreSQL的安装目录，我的是”E:\\PostgreSQL\\11”，next。 输入安装中设置的默认超级账户 postgres 密码,next。 输入模板空间数据库名字，**需要从默认名字修改为：”template_postgis”** ,install。 是否添加GDAL_DATA环境变量用来进行栅格转换，这个操作可能会覆盖掉原有GDAL_DATA环境变量，选择否，如有需要可重新添加。 栅格驱动默认未开启，是否添加环境变量来开启，选择否，如有需要可重新添加。 导出栅格功能默认未开启，是否添加环境变量来开启，选择否，如有需要可重新添加。 安装完成。 PostgreSQL(PostGIS)卸载第一步：关闭PostgreSQL服务 关闭上图中PostgreSQL服务。 第二步：卸载PostGIS拓展 运行pg安装目录下uninstall-postgis-bundle-pg11x64-3.0.2-1.exe。 Uninstall，完成。 第三步：卸载PostgreSQL 运行PostgreSQL安装目录下uninstall-postgresql.exe。 选择第一个，Next，等待进度条结束。 删除PostgreSQL安装目录下残余文件。 删除pgAdmin文件夹，位于C:\\Users&quot;username”\\AppData\\Roaming\\下。","link":"/2022/10/30/PostgreSQL(PostGIS)%E5%AE%89%E8%A3%85%E4%B8%8E%E5%8D%B8%E8%BD%BD/"},{"title":"linux自动拷贝依赖项","text":"​ 一般来说如果server在一台机器上部署好了之后再拷贝到别的机器上会出现动态库缺失的情况，需要在两台机器来回拷贝动态库十分麻烦，于是需要一个脚本，用来一键拷贝依赖的所有动态库，在需要部署的机器上会检测每个动态库是否缺失并且把缺失的动态库拷贝到指定目录下。 先放出脚本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#!/bin/bashif [ &quot;$#&quot; = &quot;0&quot; ]; then echo &quot;Error: please add option.&quot;; exit 1fiACTION=make #[make|check-and-use]#应用程序的安装目录APPDIR=/opt/app#应用程序所有依赖项的存放目录APPDEPALL=$APPDIR/dependencies/dep-all#应用程序在实际部署时缺少的动态库要拷贝到目录，这个目录要在应用程序运行前加到LD_LIBRARY_PATH环境变量里APPDEP=$APPDIR/dependencies/depwhile [ $# -gt 0 ]; do # Until you run out of parameters... case &quot;$1&quot; in --make) ACTION=make ;; --check-and-use) ACTION=check-and-use ;; --help) echo &quot;Options:&quot; echo &quot; --make [Make a dependency package]&quot; echo &quot; --check-and-use [Check and use dependent packages]&quot; echo &quot; --help [show help]&quot; exit ;; *) echo &gt;&amp;2 'copyDependencies.sh:' $&quot;unrecognized option&quot; &quot;\\`$1'&quot; echo &gt;&amp;2 $&quot;Try \\`copyDependencies.sh --help' for more information.&quot; exit 1 ;; esac shift # Check next set of parameters.doneif [ &quot;$ACTION&quot; = &quot;make&quot; ]; then #在这里可以根据自己的需要设置LD_LIBRARY_PATH,让ldd命令可以正确的找到动态库 export LD_LIBRARY_PATH=&quot;$LD_LIBRARY_PATH&quot; #下面这个命令通过管道和ldd命令搜索出来应用程序以来的所有的动态库，并将他们拷贝到APPDEPALL目录 find &quot;$APPDIR/&quot; -type f | grep -v &quot;^$APPDEPALL/&quot; | xargs -i file -F &quot; // &quot; &quot;{}&quot; | grep -E &quot; // [ ]{0,}ELF&quot; | awk -F &quot; // &quot; '{print $1}' | xargs -i ldd {} | awk -F &quot; =&gt; &quot; '{ if(NF==2) print $2}' | awk -F &quot; [(]{1}0x[0-9a-f]{16}[)]{1}$&quot; '{ if(NF==2) print $1}' | grep -v &quot;^$APPDIR/&quot; | xargs -i \\cp -L -n &quot;{}&quot; &quot;$APPDEPALL/&quot;else #ldconfig -p列出当前部署环境的动态库，如果没有则将动态库拷贝到APPDEP目录 for filename in &quot;$APPDEPALL/&quot;* ; do basefilename=`basename &quot;$filename&quot;` ldre=`ldconfig -p | grep &quot;/$basefilename\\$&quot;` if [ -z &quot;$ldre&quot; ]; then echo copy &quot;$basefilename&quot; to &quot;$APPDEP/&quot; \\cp -L -n &quot;$filename&quot; &quot;$APPDEP/&quot; fi done #有时虽然有同名的动态库，但是两个库的版本信息不一致导致程序不能启动，这是需要通过一下命令检测版本信息，如果缺少版本信息则将动态库拷贝到APPDEP目录 #以下命令参考了rpm的find-requires脚本，路径是/usr/lib/rpm/find-requires whereisobjdump=`whereis objdump` if [ ! &quot;$whereisobjdump&quot; = &quot;objdump:&quot; ]; then for checkobj in &quot;$APPDIR/server/bin/mgserver&quot; &quot;$APPDIR/webserverextensions/apache2/bin/httpd&quot; ; do reqverinfos=`objdump -p &quot;$checkobj&quot; | awk 'BEGIN { START=0; LIBNAME=&quot;&quot;; } /^[ ]{0,}required from .*:$/ { START=1; } (START==1) &amp;&amp; /required from / { sub(/:/, &quot;&quot;, $3); LIBNAME=$3; } (START==1) &amp;&amp; (LIBNAME!=&quot;&quot;) &amp;&amp; ($4!=&quot;&quot;) { print LIBNAME&quot; ----- &quot;$4; } '` echo &quot;$reqverinfos&quot; | while read reqverinfo do LIBNAME=`echo $reqverinfo | awk -F &quot; ----- &quot; '{print $1}'` VERSION=`echo $reqverinfo | awk -F &quot; ----- &quot; '{print $2}'` if [ ! X&quot;$LIBNAME&quot; = X -a ! X&quot;$VERSION&quot; = X -a ! -f &quot;$APPDEP/$LIBNAME&quot; ]; then ldre=`ldconfig -p | grep &quot;/$LIBNAME\\$&quot; | awk -F &quot; =&gt; &quot; '{print $2}'` findre=`strings &quot;$ldre&quot; | grep &quot;^$VERSION\\$&quot;` if [ -z &quot;$findre&quot; ]; then echo copy &quot;$LIBNAME&quot; to $APPDEP/ \\cp -L -n &quot;$APPDEPALL/$LIBNAME&quot; &quot;$APPDEP/&quot; fi fi done done else echo &quot;objdump executable file not found, please check the dynamic library version information by yourself&quot; fifi 解释一下 参数参数–make的作用是制作依赖项包，就是把所有的依赖项拷贝到APPDEPALL目录参数–check-and-use在部署是用，作用是在部署环境中检测哪些动态库缺失，并把缺失的动态库拷贝到APPDEP目录里。 目录APPDIR是要部署的软件的安装目录，APPDEPALL是软件所有的动态库依赖项的存放目录，APPDEP是实际部署时哪些库缺少了就把哪些库拷贝到这些目录下。为什么要有APPDEP这个目录，是因为在制作APPDEPALL时并不会检测哪些时系统的库，如果无区别的吧这些库都加到部署环境的环境变量里会导致很多基础的命令都用不了（亲测是这样的，因为一些底层的库不匹配，因此需要谨慎）。 拷贝拷贝命令有点长，解释一下 find “$APPDIR/“ -type f | grep -v “^$APPDEPALL/“查找软件安装目录下的所有文件，并过滤掉APPDEPALL目录下的动态库 find “$APPDIR/“ -type f | grep -v “^$APPDEPALL/“ | xargs -i file -F “ // “ “{}” | grep -E “ // [ ]{0,}ELF” | awk -F “ // “ ‘{print $1}’在上一步的查找结果里筛选出来动态库，根据file命令筛选 find “$APPDIR/“ -type f | grep -v “^$APPDEPALL/“ | xargs -i file -F “ // “ “{}” | grep -E “ // [ ]{0,}ELF” | awk -F “ // “ ‘{print $1}’ | xargs -i ldd {} | awk -F “ =&gt; “ ‘{ if(NF==2) print $2}’ | awk -F “ [(]{1}0x[0-9a-f]{16}[)]{1}$” ‘{ if(NF==2) print $1}’ | grep -v “^$APPDIR/“ | xargs -i \\cp -L -n “{}” “$APPDEPALL/“用ldd命令查看这些动态库的依赖项，并排除掉APPDIR目录下的动态库，最后进行拷贝 使用用ldconfig -p列出当前部署环境的动态库，如果没有则将动态库拷贝到APPDEP目录 有时虽然有同名的动态库，但是两个库的版本信息不一致导致程序不能启动，这是需要通过一下命令检测版本信息，如果缺少版本信息则将动态库拷贝到APPDEP目录，62-86行参考了rpm的find-requires脚本，路径是/usr/lib/rpm/find-requires 注意这个脚本只能尽可能包整动态库依赖的完整性，根据实际情况的不同需要人为去进行控制。","link":"/2022/10/31/linux%E8%87%AA%E5%8A%A8%E6%8B%B7%E8%B4%9D%E4%BE%9D%E8%B5%96%E9%A1%B9/"},{"title":"ElasticSearch开发手册","text":"ElasticSearch 开发手册作者:appolostar 开源的高扩展的分布式全文搜索引擎 elasticsearch与数据库的类比 关系型数据库（比如Mysql） 非关系型数据库（Elasticsearch） 数据库Database 索引Index 表Table 类型Type（7.0之后移除） 数据行Row 文档Document 数据列Column 字段Field 约束 Schema 映射Mapping 1、index、type的初衷之前es将index、type类比于关系型数据库（例如mysql）中database、table，这么考虑的目的是“方便管理数据之间的关系”。 【本来为了方便管理数据之间的关系，类比database-table 设计了index-type模型】 2、为什么现在要移除type？2.1 在关系型数据库中table是独立的（独立存储），但es中同一个index中不同type是存储在同一个索引中的（lucene的索引文件），因此不同type中相同名字的字段的定义（mapping）必须一致。 ES存入数据和搜索数据机制 (1)索引对象(blog):存储数据的表结构,任何搜索数据,存放在索引对象上(2)映射(mapping):数据如何存放在索引对象上,需要有一个映射配置,包括数据类型,是否存储,是否分词等(3)文档(document):一条数据记录,存在索引对象上(4)文档类型(type):一个索引对象,存放多种类型数据,数据用文档类型进行标识什么是分片 什么是分片 Elasticsearch集群允许系统存储的数据量超过单机容量，这是通过shard实现的。在一个索引index中，数据（document）被分片处理（sharding）到多个分片上。也就是说：每个分片都保存了全部数据中的一部分。 一个分片是一个 Lucene 的实例，它本身就是一个完整的搜索引擎。文档被存储到分片内，但应用程序直接与索引而不是与分片进行交互。 什么是副本说明 为了解决访问压力过大时单机无法处理所有请求的问题，Elasticsearch集群引入了副本策略replica。副本策略对index中的每个分片创建冗余的副本。 副本的作用如下： 提高系统容错性 当分片所在的机器宕机时，Elasticsearch可以使用其副本进行恢复，从而避免数据丢失。 提高ES查询效率 处理查询时，ES会把副本分片和主分片公平对待，将查询请求负载均衡到副本分片和主分片。 副本分片是越多越好吗？ 答案当然是 no ，原因有以下两点： （1）多个 replica 可以提升搜索操作的吞吐量和性能，但是如果只是在相同节点数目的集群上增加更多的副本分片并不能提高性能，因为每个分片从节点上获得的资源会变少，这个时候你就需要增加更多的硬件资源来提升吞吐量。 （2）更多的副本分片数提高了数据冗余量，保证了数据的完整性，但是根据上边主副分片之间的交互原理可知，分片间的数据同步会占用一定的网络带宽，影响效率，所以索引的分片数和副本数也不是越多越好。 一个节点就是集群中的一个服务器，也可以理解为，一个节点就是一个ES，作为集群的一部分，他储存数据，参与集群的索引和搜索功能，和集群类似，一个节点也是由一个名字来标识的，默认情况下，这个名字可以随意起，并且会在启动的时候赋予节点这个名字， 一个节点可以通过配置集群名称的方式来加入一个指定的集群。默认情况下，每个节点都会被安排加入到一个叫 做 “elasticsearch” 的集群中，这意味着，如果你在你的网络中启动了 若干个节点 ，并假定它们 能够相互发现彼此 ， 它们将会 自动地形成并加入 到一个叫做 “elasticsearch” 的集群中。 1.首先准备三台es服务器，我没有三台电脑，就一台电脑上启动三个es服务器，把我们原先的es文件夹复制三份， 2、 在yml文件中设置 这里我设置了三个节点组成的集群，如果想设置多个，就多创建几个服务器文件夹，然后修改里面的配置文件，并且每个node的端口号和名字都不能一样，设置各个node可以互相发现即可。 cluster-2和cluster-3和cluster-1是一样的操作。 ​ 3.当我们修改好了配置文件和创建新的服务器后，每个文件夹下的data目录里面一定要是空的，不能带任何内容。 4.我们把三个es服务器都启动 5.连接上es-header，grunt server，登陆head界面 ​ 输入我们服务器的端口号，这里我设置的是9201，9202，9203，结果都是一样的。 ​ 他们三个端口号的内容都是相同的，储存着一样的内容 一个节点在访问量和搜索量非常大的情况下，可能就会非常慢，或者某个节点中的分片负载太严重挂掉，而设置了集群，我们索引库的分片就会分散在各个分节点上，而且每个节点储存的分片是不相同的（相同的原分片和副本分片不会出现在同一个节点身上），这样就可以保证，我们大量用户搜索或者访问的时候，把我们的压力分散到每一个节点上示例1：启动2个ES节点。创建5个分片，1个副本 上图中，黄色的代表主分片，绿色的是副本。可以发现，分片与其副本不在同一个节点内。这是非常合理的，因为副本本来就是主分片的备胎，当主分片节点挂了，另外一个节点的副本将会充当主分片，如果它们在同一个节点内，副本将发挥不到作用。 7.分片 一个索引可以存储超出单个结点硬件限制的大量数据。比如，一个具有10亿文档的索引占据1TB的磁盘空间，而任 一节点都没有这样大的磁盘空间，或者单个节点处理搜索请求，响应太慢。为了解决这个问题，Elasticsearch提供 了将索引划分成多份的能力，这些份就叫做分片。当你创建一个索引的时候，你可以指定你想要的分片的数量。每个分片本身也是一个功能完善并且独立的“索引”，这个“索引”可以被放置到集群中的任何节点上。分片很重要，主要有两方面的原因：1）允许你水平分割/扩展你的内容容量。 2）允许你在分片（潜在地，位于多个节点上）之上进行分布式的、并行的操作，进而提高性能/吞吐量。 简单点来说就是，每个分片就包含了你索引库中的某一部分的内容，可以理解为一本书的目录（因为分片储存的就是索引），而分片是支持扩容的，当我们有大量的文档，由于内存不足，磁盘限制，速度极度下降，我们就需要扩容，这时一个节点就不够用了，所以我们需要把目录（分片）放到不同的空间中，也就是集群节点，这样当我们搜索某一个内容的时候，ES会把要查询的内容发给相关的分片，并将结果组合到一起。 安装基于Java语言开发的搜索引擎库类 官网：免费且开放的搜索：Elasticsearch、ELK 和 Kibana 的开发者 | Elastic 下载：Elasticsearch 8.3.2 | Elastic 解压： 配置环境： 由于es对java环境要求高，可以使用其内置的java环境，下面对其进行系统环境配置： 添加系统变量 将\\elasticsearch-8.3.2\\config下的主配置文件elasticsearch.yml进行修改： 修改： 123xpack.security.enabled: falseingest.geoip.downloader.enabled: false config/jvm.option配置文件，调整jvm堆内存大小： 修改： 123vim jvm.options-Xms4g-Xmx4g 建议不大于内存的一半 Xms和Xms设置成—样 直接运行elasticsearch.bat 运行http://localhost:9200/测试 如图效果正常 客户端Kibana安装Kibana是一个开源分析和可视化平台，旨在与Elasticsearch协同工作。 下载：Kibana 8.3.2 | Elastic 解压： 启动\\kibana-8.3.2\\bin下的kibana 访问Kibana: http://localhost:5601/ 出现该界面启动成功 ElasticSearch基本概念 传统关系型数据库和Elasticsearch的区别 在Elasticsearch中，文档归属于一种 类型(type) ,而这些类型存在于 索引(index)中，类比传统关系型数据库： Relational DB Databases Tables Rows Columns 关系型数据库 数据库 表 行 列 Elasticsearch Indices Types Documents Fields Elasticsearch 索引 类型 文档 域 在Elasticsearch中，所有的字段缺省都建了索引。 也就是说每一个字段都有一个倒排索引，用于快速查询。es支持http协议（json格式）（9200端口）、thrift、servlet、memcached、zeroMQ等的传输协议（通过插件方式集成）。传统关系型数据库不支持。es支持分片和复制，从而方便水平分割和扩展，复制保证了es的高可用与高吞吐。 索引（Index） 一个索引就是一个拥有几分相似特征的文档的集合。比如说，可以有一个客户数据的索引，另一个产品 目录的索引，还有一个订单数据的索引。 一个索引由一个名字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行 索引、搜索、更新和删除的时候，都要使用到这个名字。 文档（Document） Elasticsearch是面向文档的，文档是所有可搜索数据的最小单位。 文档会被序列化成JSON格式，保存在Elasticsearch中 JSON对象由字段组成 每个字段都有对应的字段类型(字符串/数值/布尔/日期/二进制/范围类型) 每个文档都有一个Unique ID 可以自己指定ID或者通过Elasticsearch自动生成 一篇文档包含了一系列字段，类似数据库表中的一条记录 JSON文档，格式灵活，不需要预先定义格式 字段的类型可以指定或者通过Elasticsearch自动推算 支持数组/支持嵌套 ElasticSearch索引操作创建索引 索引命名必须小写，不能以下划线开头 格式: PUT /索引名称 123456789101112131415161718#创建索引PUT /es_text#创建索引时可以设置分片数和副本数PUT /es_text{&quot;settings&quot; : {&quot;number_of_shards&quot; : 3,&quot;number_of_replicas&quot; : 2}}#修改索引配置PUT /es_text/_settings{&quot;index&quot; : {&quot;number_of_replicas&quot; : 1}} 查询索引 1234#查询索引GET /es_db#es_text是否存在HEAD /es_db 删除索引 1DELETE /es_db ElasticSearch文档操作添加文档 格式: [PUT | POST] /索引名称/[_doc | _create ]/id 1234567891011121314151617181920212223242526# 创建文档,指定id# 如果id不存在，创建新的文档，否则先删除现有文档，再创建新的文档，版本会增加PUT /es_text/_doc/1{ &quot;name&quot;: &quot;qyc&quot;, &quot;sex&quot;: 1, &quot;age&quot;: 23, &quot;address&quot;: &quot;洛阳连飞中心大厦&quot;}POST /es_text/_doc{ &quot;name&quot;: &quot;lxw&quot;, &quot;sex&quot;: 1, &quot;age&quot;: 25, &quot;address&quot;: &quot;洛阳连飞中心大厦&quot;}PUT /es_text/_create/1{ &quot;name&quot;: &quot;hxf&quot;, &quot;sex&quot;: 1, &quot;age&quot;: 25, &quot;address&quot;: &quot;洛阳连飞中心大厦&quot;} POST和PUT都能起到创建/更新的作用，PUT需要对一个具体的资源进行操作也就是要确定id才能 进行更新/创建，而POST是可以针对整个资源集合进行操作的，如果不写id就由ES生成一个唯一id进行 创建新文档，如果填了id那就针对这个id的文档进行创建/更新 Create -如果ID已经存在，会失败 修改文档 全量更新，整个json都会替换，格式: [PUT | POST] /索引名称/_doc/id 如果文档存在，现有文档会被删除，新的文档会被索引 12345678910# 全量更新，替换整个jsonPUT /es_text/_doc/1{ &quot;name&quot;: &quot;qyc&quot;, &quot;sex&quot;: 1, &quot;age&quot;: 23,}#查询文档GET /es_text/_doc/1 使用update部分更新，格式: POST /索引名称/update/id update 不会删除原来的文档，而是实现真正的数据更新 12345678910# 部分更新：在原有文档上更新# Update -文档必须已经存在，更新只会对相应字段做增量修改POST /es_text/_update/1{ &quot;doc&quot;:{ &quot;age&quot;: 24 }}GET /es_text/_doc/1 使用 _update_by_query 更新文档 12345678910111213POST /es_text/_update_by_query{ &quot;query&quot;: { &quot;match&quot;: { &quot;_id&quot;: 1 } }, &quot;script&quot;: { &quot;source&quot;: &quot;ctx._source.age = 22&quot; }}GET /es_text/_doc/1 查询文档 根据id查询文档，格式: GET /索引名称/doc/id 1GET /es_text/_doc/1 条件查询 search，格式： /索引名称/doc/_search 12# 查询前10条文档（默认）GET /es_text/_doc/_search ES Search API提供了两种条件查询搜索方式： REST风格的请求URI，直接将参数带过去 （8.x之后不再提供） 封装到request body中，这种方式可以定义更加易读的JSON格式（后面写了） 删除文档 格式: DELETE /索引名称/_doc/id 1DELETE /es_db/_doc/1 ElasticSearch文档批量操作 批量操作可以减少网络连接所产生的开销，提升性能 支持在一次API调用中，对不同的索引进行操作 可以再URI中指定Index，也可以在请求的Payload中进行 操作中单条操作失败，并不会影响其他操作 返回结果包括了每一条操作执行的结果 批量写入 批量对文档进行写操作是通过_bulk的API来实现的 请求方式：POST 请求地址：_bulk 请求参数：通过_bulk操作文档，一般至少有两行参数(或偶数行参数) 第一行参数为指定操作的类型及操作的对象(index,type和id) 第二行参数才是操作的数据 批量创建 12345POST _bulk{&quot;create&quot;:{&quot;_index&quot;:&quot;es_text&quot;,&quot;_id&quot;:4}}{&quot;name&quot;:&quot;xxs&quot;,&quot;sex&quot;:1,&quot;age&quot;:28,&quot;address&quot;:&quot;洛阳科技大厦&quot;}{&quot;create&quot;:{&quot;_index&quot;:&quot;es_text&quot;,&quot;_id&quot;:5}}{&quot;name&quot;:&quot;zmf&quot;,&quot;sex&quot;:1,&quot;age&quot;:29,&quot;address&quot;:&quot;洛阳科技中心大厦&quot;} 批量替换 如果原文档不存在，则是创建 如果原文档存在，则是替换(全量修改原文档) 12345POST _bulk{&quot;index&quot;:{&quot;_index&quot;:&quot;es_text&quot;,&quot;_id&quot;:4}}{&quot;name&quot;:&quot;xxs&quot;,&quot;sex&quot;:1,&quot;age&quot;:28,&quot;address&quot;:&quot;洛阳科技大厦&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;es_text&quot;,&quot;_id&quot;:5}}{&quot;name&quot;:&quot;zmf&quot;,&quot;sex&quot;:1,&quot;age&quot;:29,&quot;address&quot;:&quot;洛阳科技中心大厦&quot;} 批量修改 12345POST _bulk{&quot;update&quot;:{&quot;_index&quot;:&quot;es_text&quot;,&quot;_id&quot;:4}}{&quot;doc&quot;:{&quot;name&quot;:&quot;zmf&quot;}}{&quot;update&quot;:{&quot;_index&quot;:&quot;es_text&quot;,&quot;_id&quot;:5}}{&quot;doc&quot;:{&quot;name&quot;:&quot;xxs&quot;}} 批量删除 123POST _bulk{&quot;delete&quot;:{&quot;_index&quot;:&quot;es_text&quot;,&quot;_id&quot;:6}}{&quot;delete&quot;:{&quot;_index&quot;:&quot;es_text&quot;,&quot;_id&quot;:7}} 组合应用 1234POST _bulk{&quot;index&quot;:{&quot;_index&quot;:&quot;es_text&quot;,&quot;_id&quot;:6}}{&quot;name&quot;:&quot;wsq&quot;,&quot;sex&quot;:1,&quot;age&quot;:28,&quot;address&quot;:&quot;洛阳科技大厦&quot;}{&quot;delete&quot;:{&quot;_index&quot;:&quot;es_text&quot;,&quot;_id&quot;:6}} 批量读取 es的批量查询可以使用mget和msearch两种。其中mget是需要我们知道它的id，可以指定不同的 index，也可以指定返回值source。msearch可以通过字段查询来进行一个批量的查找。 1234567891011121314151617181920212223242526272829303132#可以通过ID批量获取不同index和type的数据GET _mget{&quot;docs&quot;: [{&quot;_index&quot;: &quot;es_db&quot;,&quot;_id&quot;: 1},{&quot;_index&quot;: &quot;es_text&quot;,&quot;_id&quot;: 4}]}#可以通过ID批量获取es_db的数据GET /es_text/_mget{&quot;docs&quot;: [{&quot;_id&quot;: 1},{&quot;_id&quot;: 4}]}#简化后GET /es_text/_mget{&quot;ids&quot;:[&quot;1&quot;,&quot;2&quot;]} ES检索原理分析索引的原理索引是加速数据查询的重要手段，其核心原理是通过不断的缩小想要获取数据的范围来筛选出最终想要 的结果，同时把随机的事件变成顺序的事件。 磁盘IO与预读磁盘IO是程序设计中非常高昂的操作，也是影响程序性能的重要因素，因此应当尽量避免过多的磁盘 IO，有效的利用内存可以大大的提升程序的性能。在操作系统层面，发生一次IO时，不光把当前磁盘地 址的数据，而是把相邻的数据也都读取到内存缓冲区内，局部预读性原理告诉我们，当计算机访问一个 地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。 具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发 生了一次IO，这个理论对于索引的数据结构设计非常有帮助。 ES倒排索引相当于将存入数据分词器拆分，根据词出现的频率给分，得分进行一个词组排序由高到低 为了进一步提升索引的效率索引结构 相当于将词项索引单词词典（Term Dictionary)[记录所有文档的单词，记录单词到倒排列表的关联关系] 然后根据词典找到倒排列表 (Posting List)[记录了单词对应的文档结合，由倒排索引项组成] 倒排索引项(Posting)： 文档ID 词频TF–该单词在文档中出现的次数，用于相关性评分 位置(Position)-单词在文档中分词的位置。用于短语搜索（match phrase query) 偏移(Offset)-记录单词的开始结束位置，实现高亮显示 ES高级查询Query DSLES中提供了一种强大的检索数据方式,这种检索方式称之为Query DSL（Domain Specified Language） , Query DSL是利用Rest API传递JSON格式的请求体(RequestBody)数据与ES进行交互，这种方式的丰富 查询语法让ES检索变得更强大，更简洁。 主流写法 查询所有 match_all使用match_all，默认只会返回10条数据。原因：_search查询默认采用的是分页查询，每页记录数size的默认值为10。如果想显示更多数据，指定size条数 1234567GET /es_text/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;size&quot;: 100} 分页查询formfrom 关键字: 用来指定起始返回位置，和size关键字连用可实现分页效果 12345678GET /es_db/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;size&quot;: 3, &quot;from&quot;: 0} size不能无限大 默认窗口大小为10000，该窗口大小为from数值与size之和 查询结果窗口可以对index.max_result_window进行设置 设置方法 1234PUT /es_text/_settings{ &quot;index.max_result_window&quot;:&quot;20000&quot;} 但是这样对内存的消耗巨大，引入了分页查询 分页查询Scroll 改动index.max_result_window参数值的大小，只能解决一时的问题，当索引的数据量持续增长时，在 查询全量数据时还是会出现问题。而且会增加ES服务器内存大结果集消耗完的风险。最佳实践还是根据 异常提示中的采用scroll api更高效的请求大量数据集。 123456789#查询命令中新增scroll=1m,说明采用游标查询，保持游标查询窗口一分钟。#这里由于测试数据量不够，所以size值设置为2。#实际使用中为了减少游标查询的次数，可以将值适当增大，比如设置为1000。GET /es_text/_search?scroll=1m{ &quot;query&quot;: { &quot;match_all&quot;: {}}, &quot;size&quot;: 2} 查询结果如图 返回了_scroll_id 的值 可以多次根据scroll_id游标查询，直到没有数据返回则结束查询。采用游标查询索引全量数据，更安全高 效，限制了单次对内存的消耗。 1234567# scroll_id 的值就是上一个请求中返回的 _scroll_id 的值GET /_search/scroll{ &quot;scroll&quot;:&quot;1m&quot;, &quot;scroll_id&quot;: &quot;FGluY2x1ZGVfY29udGV4dF91dWlkDXF1ZXJ5QW5kRmV0Y2gBFklLNUVUcU9qVEpDVExMMVctbGdMWWcAAAAAAAAkBRZUdEllN29zQ1RMR1VIYkd1Q2NsUWJn&quot;} 指定字段排序sort可以根据年龄进行排序 123456789101112131415161718192021222324252627GET /es_text/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;sort&quot;: [ { &quot;age&quot;: &quot;desc&quot; } ]}#排序，分页GET /es_text/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;sort&quot;: [ { &quot;age&quot;: &quot;desc&quot; } ], &quot;from&quot;: 2, &quot;size&quot;: 1} 返回指定字段 _source12345678GET /es_text/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;_source&quot;: [&quot;name&quot;,&quot;address&quot;]} 模糊匹配 matchmatch在匹配时会对所查找的关键词进行分词，然后按分词匹配查找match支持以下参数： query : 指定匹配的值operator : 匹配条件类型and : 条件分词后都要匹配or : 条件分词后有一个匹配即可(默认)minmum_should_match : 最低匹配度，即条件在倒排索引中最低的匹配度 1234567891011121314151617181920212223#模糊匹配 match 分词后or的效果GET /es_text/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;address&quot;: &quot;洛阳大厦&quot; } }}# 分词后 and的效果GET /es_text/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;address&quot;: { &quot;query&quot;: &quot;洛阳中心&quot;, &quot;operator&quot;: &quot;AND&quot; } } }} 短语查询 match_phrase1234567891011121314GET /es_text/_search{ &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;qyc洛阳&quot;, &quot;fields&quot;: [ &quot;address&quot;, &quot;name&quot; ] } }}注意：字段类型分词,将查询条件分词之后进行查询，如果该字段不分词就会将查询条件作为整体进行查询。 全字段搜索 query_string12345678910111213141516171819202122232425262728293031#未指定字段查询GET /es_text/_search{ &quot;query&quot;: { &quot;query_string&quot;: { &quot;query&quot;: &quot;xxs OR 洛阳&quot; } }}#指定单个字段查询、#Query StringGET /es_text/_search{ &quot;query&quot;: { &quot;query_string&quot;: { &quot;default_field&quot;: &quot;address&quot;, &quot;query&quot;: &quot;xxs OR 洛阳&quot; } }}#指定多个字段查询GET /es_db/_search{&quot;query&quot;: {&quot;query_string&quot;: {&quot;fields&quot;: [&quot;name&quot;,&quot;address&quot;],&quot;query&quot;: &quot;xxs OR ( 洛阳 AND 大厦)&quot;}}} 关键词查询TermTerm用来使用关键词查询(精确匹配),一般模糊查找的时候，多用 match，而精确查找时可以使用term。 ES中默认使用分词器为标准分词器(StandardAnalyzer),标准分词器对于英文单词分词,对于中文单 字分词。 在ES的Mapping Type 中 keyword , date ,integer, long , double , boolean or ip 这些类型不分 词，只有text类型分词。 12345678910111213141516171819202122232425#关键字查询 termGET /es_text/_search{ &quot;query&quot;:{ &quot;term&quot;: { &quot;address&quot;: { &quot;value&quot;: &quot;中心大厦&quot; } } }}# 采用term精确查询, 查询字段映射类型为keywordGET /es_text/_search{ &quot;query&quot;:{ &quot;term&quot;: { &quot;address.keyword&quot;: { &quot;value&quot;: &quot;中心大厦&quot; } } }}在ES中，Term查询，对输入不做分词。会将输入作为一个整体，在倒排索引中查找准确的词项，并且使用相关度算分公式为每个包含该词项的文档进行相关度算分。 前缀查询 prefix 它不会分析要搜索字符串，传入的前缀就是想要查找的前缀 默认状态下，前缀查询不做相关度分数计算，它只是将所有匹配的文档返回，然后赋予所有相关分数值为1。它的行为更像是一个过滤器而不是查询。两者实际的区别就是过滤器是可以被缓存的，而前缀查询不行。 1234567891011GET /es_text/_search{ &quot;query&quot;: { &quot;prefix&quot;: { &quot;address&quot;: { &quot;value&quot;: &quot;洛阳&quot; } } }} 通配符查询 wildcard1234567891011GET /es_text/_search{ &quot;query&quot;: { &quot;wildcard&quot;: { &quot;address&quot;: { &quot;value&quot;: &quot;*飞*&quot; }· } }} 范围查询 rangerange：范围关键字gte 大于等于lte 小于等于gt 大于lt 小于now 当前时间 1234567891011POST /es_db/_search{ &quot;query&quot;: { &quot;range&quot;: { &quot;age&quot;: { &quot;gte&quot;: 25, &quot;lte&quot;: 28 } } }} 日期 range1234567891011121314151617181920DELETE /productPOST /product/_bulk{&quot;index&quot;:{&quot;_id&quot;:1}}{&quot;price&quot;:100,&quot;date&quot;:&quot;2021-01-01&quot;,&quot;productId&quot;:&quot;XHDK-1293&quot;}{&quot;index&quot;:{&quot;_id&quot;:2}}{&quot;price&quot;:200,&quot;date&quot;:&quot;2022-01-01&quot;,&quot;productId&quot;:&quot;KDKE-5421&quot;}GET /product/_mappingGET /product/_search{ &quot;query&quot;: { &quot;range&quot;: { &quot;date&quot;: { &quot;gte&quot;: &quot;now-2y&quot; } } }} 多 id 查询 ids123456789GET /es_db/_search{ &quot;query&quot;: { &quot;ids&quot;: { &quot;values&quot;: [1,2] } }} 模糊查询 fuzzy在实际的搜索中，我们有时候会打错字，从而导致搜索不到。在Elasticsearch中，我们可以使用fuzziness属性来进行模糊查询，从而达到搜索有错别字的情形。fuzzy 查询会用到两个很重要的参数，fuzziness，prefix_length fuzziness：表示输入的关键字通过几次操作可以转变成为ES库里面的对应field的字段操作是指：新增一个字符，删除一个字符，修改一个字符，每次操作可以记做编辑距离为1，如中文集团到中威集团编辑距离就是1，只需要修改一个字符；该参数默认值为0，即不开启模糊查询。如果fuzziness值在这里设置成2，会把编辑距离为2的东东集团也查出来。prefix_length：表示限制输入关键字和ES对应查询field的内容开头的第n个字符必须完全匹配，不允许错别字匹配如这里等于1，则表示开头的字必须匹配，不匹配则不返回默认值也是0 12345678910111213141516171819202122232425262728GET /es_text/_search{ &quot;query&quot;: { &quot;fuzzy&quot;: { &quot;address&quot;: { &quot;value&quot;: &quot;罗阳&quot;, &quot;fuzziness&quot;: 1 } } }}GET /es_text/_search{ &quot;query&quot;: { &quot;fuzzy&quot;: { &quot;address&quot;: { &quot;value&quot;: &quot;科及&quot;, &quot;fuzziness&quot;: 1 } } }}注意: fuzzy 模糊查询 最大模糊错误 必须在0-2之间- 搜索关键词长度为 2，不允许存在模糊- 搜索关键词长度为3-5，允许1次模糊- 搜索关键词长度大于5，允许最大2次模糊 高亮 highlighthighlight 关键字: 可以让符合条件的文档中的关键词高亮。 pre_tags 前缀标签 post_tags 后缀标签 tags_schema 设置为styled可以使用内置高亮样式 require_field_match 多字段高亮需要设置为false 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687示例代码#指定ik分词器（7.29没有8.3.2版本的需要等待插件更新）PUT /products{ &quot;settings&quot; : { &quot;index&quot; : { &quot;analysis.analyzer.default.type&quot;: &quot;ik_max_word&quot; } }}PUT /products/_doc/1{ &quot;proId&quot; : &quot;2&quot;, &quot;name&quot; : &quot;牛仔男外套&quot;, &quot;desc&quot; : &quot;牛仔外套男装春季衣服男春装夹克修身休闲男生潮牌工装潮流头号青年春秋棒球服男 7705浅蓝常规 XL&quot;, &quot;timestamp&quot; : 1576313264451, &quot;createTime&quot; : &quot;2019-12-13 12:56:56&quot;}PUT /products/_doc/2{ &quot;proId&quot; : &quot;6&quot;, &quot;name&quot; : &quot;HLA海澜之家牛仔裤男&quot;, &quot;desc&quot; : &quot;HLA海澜之家牛仔裤男2019时尚有型舒适HKNAD3E109A 牛仔蓝(A9)175/82A(32)&quot;, &quot;timestamp&quot; : 1576314265571, &quot;createTime&quot; : &quot;2019-12-18 15:56:56&quot;}测试GET /products/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;牛仔&quot; } } }, &quot;highlight&quot;: { &quot;fields&quot;: { &quot;*&quot;:{} } }}自定义高亮 html 标签可以在 highlight 中使用 pre_tags 和 post_tagsGET /products/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;牛仔&quot; } } }, &quot;highlight&quot;: { &quot;post_tags&quot;: [&quot;&lt;/span&gt;&quot;], &quot;pre_tags&quot;: [&quot;&lt;span style='color:red'&gt;&quot;], &quot;fields&quot;: { &quot;*&quot;:{} } }}多字段高亮GET /products/_search{ &quot;query&quot;: { &quot;term&quot;: { &quot;name&quot;: { &quot;value&quot;: &quot;牛仔&quot; } } }, &quot;highlight&quot;: { &quot;pre_tags&quot;: [&quot;&lt;font color='red'&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;font/&gt;&quot;], &quot;require_field_match&quot;: &quot;false&quot;, &quot;fields&quot;: { &quot;name&quot;: {}, &quot;desc&quot;: {} } }} 相关性和相关性算分搜索是用户和搜索引擎的对话，用户关心的是搜索结果的相关性是否可以找到所有相关的内容有多少不相关的内容被返回了文档的打分是否合理结合业务需求，平衡结果排名如何衡量相关性：Precision(查准率)―尽可能返回较少的无关文档Recall(查全率)–尽量返回较多的相关文档Ranking -是否能够按照相关度进行排序 相关性（Relevance）搜索的相关性算分，描述了一个文档和查询语句匹配的程度。ES 会对每个匹配查询条件的结果进行算分_score。打分的本质是排序，需要把最符合用户需求的文档排在前面。ES 5之前，默认的相关性算分采用TF-IDF，现在采用BM 25。 什么是TF-IDFTF-IDF（term frequency–inverse document frequency）是一种用于信息检索与数据挖掘的常用加权技术。 BM25es5之后用的是这个算分 和经典的TF-IDF相比,当TF无限增加时，BM 25算分会趋于一个数值 通过Explain API查看TF-IDF1234567891011121314151617181920PUT /test_score/_bulk{&quot;index&quot;:{&quot;_id&quot;:1}}{&quot;content&quot;:&quot;we use Elasticsearch to power the search&quot;}{&quot;index&quot;:{&quot;_id&quot;:2}}{&quot;content&quot;:&quot;we like elasticsearch&quot;}{&quot;index&quot;:{&quot;_id&quot;:3}}{&quot;content&quot;:&quot;Thre scoring of documents is caculated by the scoring formula&quot;}{&quot;index&quot;:{&quot;_id&quot;:4}}{&quot;content&quot;:&quot;you know,for search&quot;}GET /test_score/_search{ &quot;explain&quot;: true, &quot;query&quot;: { &quot;match&quot;: { &quot;content&quot;: &quot;elasticsearch&quot; } }} BoostingBoosting是控制相关度的一种手段。参数boost的含义： 当boost &gt; 1时，打分的权重相对性提升当0 &lt; boost &lt;1时，打分的权重相对性降低当boost &lt;0时，贡献负分返回匹配positive查询的文档并降低匹配negative查询的文档相似度分。这样就可以在不排除某些文档的前提下对文档进行查询,搜索结果中存在只不过相似度分数相比正常匹配的要低; 12345678910111213141516171819GET /test_score/_search{ &quot;query&quot;: { &quot;boosting&quot;: { &quot;positive&quot;: { &quot;term&quot;: { &quot;content&quot;: &quot;elasticsearch&quot; } }, &quot;negative&quot;: { &quot;term&quot;: { &quot;content&quot;: &quot;like&quot; } }, &quot;negative_boost&quot;: 0.2 } }} 希望包含了某项内容的结果不是不出现，而是排序靠后。 布尔查询bool Query一个bool查询,是一个或者多个查询子句的组合，总共包括4种子句，其中2种会影响算分，2种不影响算分。 must: 相当于&amp;&amp; ，必须匹配，贡献算分should: 相当于|| ，选择性匹配，贡献算分must_not: 相当于! ，必须不能匹配，不贡献算分filter: 必须匹配，不贡献算法在Elasticsearch中，有Query和 Filter两种不同的Context Query Context: 相关性算分Filter Context: 不需要算分 ,可以利用Cache，获得更好的性能相关性并不只是全文本检索的专利，也适用于yes | no 的子句，匹配的子句越多，相关性评分越高。如果多条查询子句被合并为一条复合查询语句，比如 bool查询，则每个查询子句计算得出的评分会被合并到总的相关性评分中。 Boosting QueryBoosting是控制相关的一种手段。可以通过指定字段的boost值影响查询结果 参数boost的含义： 当boost &gt; 1时，打分的权重相对性提升 当0 &lt; boost &lt;1时，打分的权重相对性降低 当boost &lt;0时，贡献负分 1234567891011121314151617181920212223242526272829303132POST /blogs/_bulk{&quot;index&quot;:{&quot;_id&quot;:1}}{&quot;title&quot;:&quot;Apple iPad&quot;,&quot;content&quot;:&quot;Apple iPad,Apple iPad&quot;}{&quot;index&quot;:{&quot;_id&quot;:2}}{&quot;title&quot;:&quot;Apple iPad,Apple iPad&quot;,&quot;content&quot;:&quot;Apple iPad&quot;}GET /blogs/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ { &quot;match&quot;: { &quot;title&quot;: { &quot;query&quot;: &quot;apple,ipad&quot;, &quot;boost&quot;: 1 } } }, { &quot;match&quot;: { &quot;content&quot;: { &quot;query&quot;: &quot;apple,ipad&quot;, &quot;boost&quot;: 4 } } } ] } }} 利用negative_boost降低相关性negative_boost 对 negative部分query生效计算评分时,boosting部分评分不修改，negative部分query乘以negative_boost值negative_boost取值:0-1.0，举例:0.3对某些返回结果不满意，但又不想排除掉（ must_not)，可以考虑boosting query的negative_boost。 12345678910111213141516171819GET /news/_search{ &quot;query&quot;: { &quot;boosting&quot;: { &quot;positive&quot;: { &quot;match&quot;: { &quot;content&quot;: &quot;apple&quot; } }, &quot;negative&quot;: { &quot;match&quot;: { &quot;content&quot;: &quot;pie&quot; } }, &quot;negative_boost&quot;: 0.2 } }} 常用Mapping参数配置index: 控制当前字段是否被索引，默认为true。如果设置为false，该字段不可被搜索 123456789101112131415161718192021222324252627282930313233DELETE /userPUT /user{&quot;mappings&quot; : { &quot;properties&quot; : { &quot;address&quot; : { &quot;type&quot; : &quot;text&quot;, &quot;index&quot;: false }, &quot;age&quot; : { &quot;type&quot; : &quot;long&quot; }, &quot;name&quot; : { &quot;type&quot; : &quot;text&quot; } }}}PUT /user/_doc/1{&quot;name&quot;:&quot;fox&quot;,&quot;address&quot;:&quot;洛阳洛龙&quot;,&quot;age&quot;:30}GET /userGET /user/_search{&quot;query&quot;: {&quot;match&quot;: {&quot;address&quot;: &quot;洛阳洛龙&quot;}}} 无法找到 Dynamic Template根据Elasticsearch识别的数据类型，结合字段名称，来动态设定字段类型 所有的字符串类型都设定成Keyword，或者关闭keyword 字段 is开头的字段都设置成 boolean long_开头的都设置成 long类型 123456789101112131415161718192021222324252627282930313233PUT /my_test_index{&quot;mappings&quot;: {&quot;dynamic_templates&quot;: [{&quot;full_name&quot;:{&quot;path_match&quot;: &quot;name.*&quot;,&quot;path_unmatch&quot;: &quot;*.middle&quot;,&quot;mapping&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;copy_to&quot;: &quot;full_name&quot;}}}]}}PUT /my_test_index/_doc/1{&quot;name&quot;:{&quot;first&quot;: &quot;John&quot;,&quot;middle&quot;: &quot;Winston&quot;,&quot;last&quot;: &quot;Lennon&quot;}}GET /my_test_index/_search{&quot;query&quot;: {&quot;match&quot;: {&quot;full_name&quot;: &quot;John&quot;}}} 12345678910111213141516171819202122232425PUT /my_index{ &quot;mappings&quot;: { &quot;dynamic_templates&quot;: [ { &quot;strings_as_boolean&quot;: { &quot;match_mapping_type&quot;:&quot;string&quot;, &quot;match&quot;:&quot;is*&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;boolean&quot; } } }, { &quot;strings_as_keywords&quot;: { &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;keyword&quot; } } } ] }} lndex Template当一个索引被新创建时： 应用Elasticsearch 默认的settings 和mappings 应用order数值低的lndex Template 中的设定 应用order高的 Index Template 中的设定，之前的设定会被覆盖 应用创建索引时，用户所指定的Settings和 Mappings，并覆盖之前模版中的设定","link":"/2022/11/02/ElasticSearch%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","link":"/tags/PostgreSQL/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"运维","slug":"运维","link":"/tags/%E8%BF%90%E7%BB%B4/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/tags/ElasticSearch/"}],"categories":[{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"PostgreSQL","slug":"数据库/PostgreSQL","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/PostgreSQL/"},{"name":"ElasticSearch","slug":"数据库/ElasticSearch","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/"}],"pages":[]}